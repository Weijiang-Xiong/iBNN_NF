\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage[round]{natbib}
\bibliographystyle{plainnat}

\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}

\usepackage{amsmath,amsthm,amssymb}
\usepackage{algorithm,algorithmic}
\usepackage{graphicx,pgfgantt,xcolor}
\usepackage{pifont}

\definecolor{linkblue}{rgb}{0.10,0.40,0.70}

\usepackage[colorlinks=true,citecolor=linkblue]{hyperref}

\newcommand{\cmark}{ \textcolor{green!60!black}{\ding{51}} }
\newcommand{\xmark}{ \textcolor{red!60!black}{\ding{55}} }
\usepackage{url}

\def\f{\mathbf{f}}
\def\x{\mathbf{x}}
\def\bX{\mathbf{X}}
\def\F{\mathbf{F}}
\def\A{\mathbf{A}}
\def\xt{\tilde{\mathbf{x}}}
\def\t{\mathbf{t}}
\def\s{\mathbf{s}}
\def\D{\mathcal{D}}
\def\a{\mathbf{a}}
\def\W{\mathbf{W}}
\def\ds{\dot{\mathbf{s}}}
\def\ddx{\ddot{\mathbf{x}}}
\def\dy{\dot{\mathbf{y}}}
\def\da{\dot{\mathbf{a}}}
\def\e{\mathbf{e}}
\def\h{\mathbf{h}}
\def\bd{\boldsymbol{\delta}}
\def\bE{\boldsymbol{\mathcal{E}}}
\def\dx{\dot{\mathbf{x}}}
\def\B{\mathbf{B}}
\def\bt{\boldsymbol{\theta}}
\def\th{{\theta}}
\def\c{\mathbf{c}}
\def\J{\mathbf{J}}
\def\z{\mathbf{z}}
\def\cZ{\mathcal{Z}}
\def\dv{\dot{\mathbf{v}}}
\def\dhx{\dot{\hat{\mathbf{x}}}}
\def\hf{\hat{\mathbf{f}}}
\def\hx{{\hat{\mathbf{x}}}}
\def\g{\mathbf{g}}
\def\Z{\mathbf{Z}}
\def\V{\mathbf{V}}
\def\U{\mathbf{U}}
\def\tx{\tilde{\mathbf{x}}}
\def\w{\mathbf{w}}
\def\m{\mathbf{m}}
\def\Gamma{\mathrm{Gamma}}
\def\k{\mathbf{k}}
\def\p{\mathbf{p}}
\def\dbl{\dot{\boldsymbol{\lambda}}} 
\def\r{\mathbf{r}}
\def\1{\mathbf{1}}
\def\R{\mathbb{R}}
\def\X{\mathcal{X}}
\def\Q{\mathbf{Q}}
\def\S{\mathbf{S}}
\def\L{\mathcal{L}}
\def\K{\mathbf{K}}
\def\cW{\mathcal{W}}
\def\bo{\boldsymbol{\omega}}
\def\y{\mathbf{y}}
\def\u{\mathbf{u}}
\def\bmu{\boldsymbol\mu}
\def\bl{{\boldsymbol\lambda}}
\def\bt{{\boldsymbol\theta}}
\def\bomega{\boldsymbol\omega}
\def\bL{\mathbf\Lambda}
\def\bS{\mathbf\Sigma}
\def\bR{\mathbf{R}}
\def\btau{\boldsymbol\tau}
\def\bxi{\boldsymbol\xi}
\def\bphi{\boldsymbol\phi}
\def\v{{\mathbf{v}}}
\def\be{\boldsymbol\varepsilon}
\def\bs{\boldsymbol\sigma}
%\def\e{\varepsilon}
\def\0{\mathbf{0}}
\def\N{\mathcal{N}}
\def\E{\mathbb{E}}
\def\cov{\mathbf{cov}}
\def\var{\mathbf{var}}
\def\GP{\mathcal{GP}}
\def\diag{\mathrm{diag}}
\def\tr{\mathrm{tr}}
\def\kl{\mathrm{KL}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\orange}[1]{\textcolor{orange}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\green}[1]{\textcolor{green}{#1}}



\title{Multimodal Bayesian neural networks}
\author{Weijiang Xiong, Markus Heinonen, Samuel Kaski}
\date{\today}

\begin{document}

\maketitle

This is the running document of the summer intern project 2021 of Weijiang Xiong on Bayesian neural networks. The work is supervised by PhD Markus Heinonen under Prof. Samuel Kaski and with help of Msc Trinh Trung. 

\paragraph{TLDR:}
Develop multimodal inference for BNNs.

\section{Research diary}

\red{TODO for Weijiang}

\section{Problem definition}

\subsection{BNN setting}

Bayesian neural networks are a probabilistic variants of neural networks, where conventionally we place a prior $p(\w)$ on the neural weights $\w$, which results in a posterior distribution $p(\w|\D)$ of the weights given data $\D = \{\x_i,\y_i\}_{i=1}^N$. We assume that the weights $\w$ contains all parameters of the network, including biases, filter kernels, etc. 

While conventional DNNs only account for a single function hypotheses $f(\x ; \w_{\text{opt}})$, BNNs can capture a large distribution of alternative function $f$ hypotheses by considering multiple convolution, weight and pattern combinations. This is described by the predictive posterior
\begin{align}
    p(\y_* | \x_*, \D) &= \int \underbrace{p(\y_* | \x_*, \w)}_{\text{likelihood}} p(\w | \D) d\w,
\end{align}
where we average predictions for a new test point $(\x_*,\y_*)$ from all posterior weights. This generally has been shown to improve calibration, uncertainty quantification, out-of-distribution prediction and robustness. For excellent overviews of BNNs, we refer to Wilson's work \citep{wilson2020,wilson2020b}. The loss landscapes have been studied by \citet{garipov18}

Most of BNN research has focused on five major research questions
\begin{itemize}
    \item What type of priors should be used?
    \item How to do model selection (that is, learn hyperparameters)
    \item How to numerically infer the posterior? 
    \item How should we parameterise the neural function?
    \item What are theoretical connections of probabilistic neural networks to other models? 
\end{itemize}

In this summer project we focus on the inference problem (and also a bit on the parameterisation).

\subsection{Variational inference for BNNs (BNN-VI)}

The currently dominant BNN approach uses Gaussian priors $p(\w) = \N(\0, \sigma^2 I)$ for the weights, and applies mean-field variational inference (MFVI) \citep{blundell2015weight}. In MFVI full posterior inference is sidestepped by introducing a tractable variational posterior approximation $q(\w)$ such that its Kullback Leibler distance to the true posterior is minimized,
\begin{align}
    \argmin_{\w} \: \kl[ q(\w) || p(\w | \D)].
\end{align}
Under variational inference (See \citet{blei2017}) it can be shown that this intractable KL is (remarkably!) equivalent to maximizing the evidence lower bound
\begin{align}
    \log p(\D) \ge \underbrace{\E_{q(\w)} \log p(\D | \w)}_{\text{variational likelihood}} - \underbrace{\kl[ q(\w) || p(\w)]}_{\text{KL term}},
\end{align}
where we now maximize the variational likelihood and minimize the KL term between the posterior approximation $q(\w)$ and the weight prior $p(\w)$. By assuming factorised Gaussians
\begin{align}
    q(\w) &= \N(\w | \m, \diag \, \s^2) \\
    &= \prod_i \N(w_i | m_i, s_i^2) \\
    p(\w) &= \N(\w | \0, \sigma^2 I) \\
    &= \prod_i \N(w_i | 0, \sigma^2)
\end{align}
both the KL term and the variational likelihood term become easy to compute and optimise (via the reparameterisation trick). 

\subsection{Alternative inference methods}

While the MFVI-BNN is the 'standard' approach, alternative approaches have also been proposed. Full-batch HMC samples directly from the extremely complex posterior landscape $p(\w|\D)$ \citep{izmailov2021}, which is highly impractical. Earlier \citet{wenzel2020good} proposed more practical minibatch-based SG-HMC to infer the same posterior, and found out the need to apply tempering to downplay the prior. The celebrated method SWAG uses the SGD optimisation trace to estimate the shape of the local optimum the SGD is converging towards, and places a Gaussian approximation on it (This method comes standard in pytorch 1.6\footnote{ https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/}) \citep{maddox2019simple}. MultiSWAG repeats the procedure for multiple initialisations following the deep ensembles. Early works have also used normalizing flows to estimate possibly multimodal posteriors \citep{louizos2017}.

\subsection{Low-rank parameterisations}

The inference of the posterior $p(\w|\D)$ is inherently extremely difficult to the weights being often up to 100-million dimensional vectors. Exploration of the loss landscape in such high-dimensional spaces is possibly a futile task. A series of works have sought low-rank parameterisations of neural networks \citep{dusenberry2020,karaletsos2018probabilistic,karaletsos2020hierarchical,trung2020}. Multiple authors have proposed learning multiplicative latent \emph{node} variables,
\begin{align}
    \x_{\ell+1} = \sigma( W_\ell (\x_\ell \circ \z_\ell) + \mathbf{b}_\ell),
\end{align}
where the node activations $\x_\ell$ of layer $\ell$ are multiplied by stochastic random variables $\z$. By defining the weights are deterministic parameters, we only need to infer the posterior $p(\z|\D)$ of a much smaller latent variable space. 

\subsection{Problem definition}

The topic of the summer project is to develop multimodal posterior inference for BNNs in combination with a node parameterisation.

\paragraph{Multimodal inference.} We need to select a way to perform multimodal inference. We invented a new mixture-of-Gaussian variational approximation in our iBNN paper \citep{trung2020}, which however does not explore many modes. Conventional approximate inference literature would propose the boosting VI approach, or even active learning. The normalizing flows are a great candidate for multimodality, however there are numerous NF models to choose from \citep{papa21}. 

\paragraph{Node parameterisations.} We can start from the iBNN code and its node parameters, but alternatives also exist (such as rank-1 BNN \citep{dusenberry2020}).

\paragraph{Success criteria.} We want to compare our method against BNN literature on MNIST, Fashion-MNIST and CIFAR problems. We want to showcase the improvements of the multimodal inference in accuracy (error), calibration (ECE, log-likelihood), robustness (corruption) and out-of-distribution performance. Our main method to beat deep ensembles, and we want to show how the degree of multimodality correlates with better results.

\subsection{How to start}

At the start of the project
\begin{itemize}
    \item Let's invite and setup PML slack for the project, and regular project meetings
    \item Test and play around with the iBNN codebase and Aalto's computing clusters. Familiarize yourself with the code and repeat some of the experiments of the iBNN as a demonstration
    \item Read iBNN paper \citep{trung2020} and Wilson's two generalisation papers \citep{izmailov2021,wilson2020b}
    \item Try SGHMC as baseline approach: how does it work? Use an existing implementation to do this (eg. \citet{wenzel2020good}).
    \item Implement in pytorch the multimodal inference (eg. normalizing flow)
\end{itemize}


\bibliography{refs}





\end{document}
